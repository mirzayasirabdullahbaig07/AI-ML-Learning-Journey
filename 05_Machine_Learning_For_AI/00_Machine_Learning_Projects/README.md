# ğŸ¤– Internship Projects on Machine Learning & Deep Learning

Welcome to my internship project repository! ğŸ“  
This repository contains two hands-on projects focused on **Machine Learning** and **Deep Learning** techniques, developed as part of my internship to gain real-world experience in data analysis, time series forecasting, and large language model fine-tuning.

---

## ğŸ“Œ Project 1: Ethereum (ETH/USDT) Price Forecasting using ARIMA

A complete **Time Series Analysis** project to forecast cryptocurrency prices using the ARIMA model.

### Key Highlights:
- Dataset: ETH/USDT daily price data from Kaggle.
- Techniques: Data preprocessing, EDA, ADF test, ACF/PACF plots.
- Model: ARIMA (AutoRegressive Integrated Moving Average).
- Evaluation: RMSE, MAPE, RÂ² score.
- Output: 30-day price forecast with visualization.

ğŸ“‚ Project Link: `ETH_ARIMA_Forecast.ipynb`  
ğŸ“„ Detailed ReadMe: [Click here](./ETH_ARIMA_Forecast/README.md)

---

## ğŸ“Œ Project 2: Parameter-Efficient Fine-Tuning of LLaMA 3.2 on Medical Chain-of-Thought Dataset

A **Deep Learning** project that fine-tunes the **LLaMA 3.2 (3B)** model using a Medical Chain-of-Thought dataset with **parameter-efficient tuning (LoRA)** in **Unsloth**.

### Key Highlights:
- Platform: Kaggle Notebooks with GPU.
- Dataset: Medical Chain-of-Thought from Hugging Face.
- Model: LLaMA 3.2 (3B) with 4-bit quantization.
- Fine-tuning Method: LoRA via Unsloth.
- Tracking: wandb for training metrics.
- Evaluation: ROUGE-L score before and after fine-tuning.
- Output: Fine-tuned model adapter uploaded to Hugging Face.

ğŸ“‚ Project Link: `LLaMA3.2_Medical_CoT_FineTuning.ipynb`  
ğŸ“„ Detailed ReadMe: [Click here](./LLaMA3.2_Medical_CoT_FineTuning/README.md)

---

## ğŸ§° Tools & Libraries Used

- Python, Pandas, NumPy
- Matplotlib, Seaborn, Statsmodels, Sklearn
- Hugging Face Transformers
- Unsloth, LoRA, wandb
- Kaggle Notebooks, JupyterLab

---

## ğŸ“š Goal of These Projects

âœ” Strengthen hands-on knowledge in Machine Learning (ARIMA)  
âœ” Apply Deep Learning with Large Language Models (LLaMA 3.2)  
âœ” Learn efficient model tuning with minimal GPU usage  
âœ” Practice model evaluation and reporting  
âœ” Build end-to-end AI pipelines from data to deployment

---

## ğŸ™‹â€â™‚ï¸ About Me

**[Your Name]**  
ğŸ“Œ Computer Science Graduate | Machine Learning & AI Enthusiast  
ğŸ“§ [Your Email]  
ğŸŒ GitHub: [your-github-profile]  
ğŸ§  Interested in: LLMs, Time Series Forecasting, NLP, and AI Applications in Healthcare

---

## ğŸ”— Let's Connect

If you're interested in collaborating, learning together, or giving feedback, feel free to reach out!

